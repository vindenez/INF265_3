{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import json\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(file_path):\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # Preprocess the text by removing punctuation and converting to lowercase\n",
    "    text = preprocess_text(text)\n",
    "    return tokenizer(text)\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "# Function to tokenize all files in a given directory\n",
    "def tokenize_directory(directory_path):\n",
    "    tokenized_texts = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.txt'):  # Make sure to process only text files\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            tokens = read_and_tokenize(file_path)\n",
    "            tokenized_texts.extend(tokens)\n",
    "    return tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in training data: 2348007\n",
      "Number of words in validation data: 43758\n",
      "Number of words in test data: 107587\n"
     ]
    }
   ],
   "source": [
    "# Assuming your directories are named as follows:\n",
    "train_folder = 'inf265_v24_project03_data/data_train'\n",
    "val_folder = 'inf265_v24_project03_data/data_val'\n",
    "test_folder = 'inf265_v24_project03_data/data_test'\n",
    "\n",
    "# Tokenize all texts in each directory\n",
    "tokenized_train = tokenize_directory(train_folder)\n",
    "tokenized_val = tokenize_directory(val_folder)\n",
    "tokenized_test = tokenize_directory(test_folder)\n",
    "\n",
    "# Now you have lists of words for each dataset\n",
    "print(f\"Number of words in training data: {len(tokenized_train)}\")\n",
    "print(f\"Number of words in validation data: {len(tokenized_val)}\")\n",
    "print(f\"Number of words in test data: {len(tokenized_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_texts, min_freq=100):\n",
    "    word_freq = Counter(tokenized_texts)\n",
    "    vocab = {'<unk>': 0}  # Start with the '<unk>' token\n",
    "    index = 1  # Start indexing from 1 for other words\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "    total_words = sum(word_freq.values())\n",
    "    distinct_words = len(word_freq)\n",
    "    vocab_size = len(vocab)\n",
    "    return vocab, total_words, distinct_words, vocab_size\n",
    "\n",
    "vocab, total_words, distinct_words, vocab_size = build_vocabulary(tokenized_train, min_freq=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset: 2348007\n",
      "Number of distinct words in the training dataset: 52585\n",
      "Size of the defined vocabulary (words appearing at least 100 times): 2082\n",
      "\n",
      "Comments on Results:\n",
      "The defined vocabulary is smaller than the total number of distinct words,\n",
      "indicating that not all words appear frequently enough to be included.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of words in the training dataset: {total_words}\")\n",
    "print(f\"Number of distinct words in the training dataset: {distinct_words}\")\n",
    "print(f\"Size of the defined vocabulary (words appearing at least 100 times): {vocab_size}\")\n",
    "\n",
    "print(\"\\nComments on Results:\")\n",
    "if vocab_size < distinct_words:\n",
    "    print(\"The defined vocabulary is smaller than the total number of distinct words,\")\n",
    "    print(\"indicating that not all words appear frequently enough to be included.\")\n",
    "else:\n",
    "    print(\"All distinct words appear frequently and are included in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)  # Projection layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Inputs is a list of context word indices for each target word\n",
    "        embeds = self.embeddings(inputs)  # Convert word indices to embeddings\n",
    "        embeds_mean = torch.mean(embeds, dim=1)  # Take the mean across the context window\n",
    "        out = self.linear(embeds_mean)  # Make a prediction for the target word\n",
    "        log_probs = F.log_softmax(out, dim=1)  # Use log_softmax to convert to log probabilities\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, word_to_ix, context_size):\n",
    "        self.word_to_ix = word_to_ix  # Store the mapping as an attribute\n",
    "        self.data = []\n",
    "        for i in range(context_size, len(tokenized_texts) - context_size):\n",
    "            context = [tokenized_texts[j] for j in range(i - context_size, i + context_size + 1) if j != i]\n",
    "            target = tokenized_texts[i]\n",
    "            context_indices = [word_to_ix.get(word, word_to_ix['<unk>']) for word in context]\n",
    "            target_index = word_to_ix.get(target, word_to_ix['<unk>'])\n",
    "            self.data.append((context_indices, target_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context_indices, target_index = self.data[idx]\n",
    "        return torch.tensor(context_indices, dtype=torch.long), torch.tensor(target_index, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "train_data = CBOWDataset(tokenized_train, word_to_ix, context_size=2)\n",
    "val_data = CBOWDataset(tokenized_val, word_to_ix, context_size=2)\n",
    "\n",
    "# Compute class weights\n",
    "#class_weights = compute_class_weights(word_to_ix, train_data)\n",
    "\n",
    "# Prepare the DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Store each model's validation loss to select the best model\n",
    "model_losses = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights calculated successfully: [3.01304916e-03 7.45505344e-03 1.71758027e-02 ... 5.78340107e+00\n",
      " 4.66017855e+00 4.33755080e+00]\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(word_to_ix, dataset):\n",
    "    # Extract all target indices from the dataset to a numpy array\n",
    "    all_targets = np.array([target for _, target in dataset])\n",
    "    \n",
    "    # Ensure classes contain all possible labels from the vocabulary\n",
    "    class_indices = np.array(list(word_to_ix.values()))  # Use all word indices from your vocabulary\n",
    "    \n",
    "    # Calculate class weights using sklearn's utility function\n",
    "    class_weights = compute_class_weight('balanced', classes=class_indices, y=all_targets)\n",
    "    \n",
    "    # Print and return the class weights\n",
    "    print(\"Class weights calculated successfully:\", class_weights)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "class_weights = compute_class_weights(word_to_ix, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow_models(vocab_size, embedding_dims, epochs, learning_rate):\n",
    "    for embedding_dim in embedding_dims:\n",
    "        model = CBOWModel(vocab_size, embedding_dim).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for context, target in train_loader:\n",
    "                context, target = context.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                log_probs = model(context)\n",
    "                loss = criterion(log_probs, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            average_train_loss = total_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch}, Average Training Loss: {average_train_loss}')\n",
    "\n",
    "        model_path = f'cbow_model_{embedding_dim}.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for context, target in val_loader:\n",
    "                context, target = context.to(device), target.to(device)\n",
    "                log_probs = model(context)\n",
    "                loss = criterion(log_probs, target)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        model_losses[embedding_dim] = avg_val_loss\n",
    "        print(f'Model with embedding dim {embedding_dim}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    # Select the best model\n",
    "    best_embedding_dim = min(model_losses, key=model_losses.get)\n",
    "    print(f'The best model has an embedding dimension of {best_embedding_dim} with a validation loss of {model_losses[best_embedding_dim]}')\n",
    "\n",
    "    return best_embedding_dim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "context_size = 2  # Set your context size\n",
    "embedding_dims = [10, 12, 16]  # The list of embedding dimensions you want to try\n",
    "epochs = 5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenized_text.json', 'w') as f:\n",
    "    json.dump(tokenized_train, f)\n",
    "\n",
    "with open('vocabulary.json', 'w') as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Training Loss: 7.078325357692656\n",
      "Epoch 1, Average Training Loss: 6.967803973049848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m train_cbow_models(vocab_size, embedding_dims, epochs, learning_rate)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe best model uses an embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m, in \u001b[0;36mtrain_cbow_models\u001b[0;34m(vocab_size, embedding_dims, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(context)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(log_probs, target)\n\u001b[0;32m---> 15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = train_cbow_models(vocab_size, embedding_dims, epochs, learning_rate)\n",
    "\n",
    "print(f\"The best model uses an embedding dimension of {best_model}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "embeddings = best_model.embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Normalize the embeddings to make them unit vectors\n",
    "# This step is necessary because cosine similarity is a measure of orientation, not magnitude\n",
    "norm_embeddings = embeddings / embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(norm_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_words(word, word_to_ix, ix_to_word, similarity_matrix, top_n=10):\n",
    "    word_index = word_to_ix[word]\n",
    "    word_similarities = similarity_matrix[word_index]\n",
    "    \n",
    "    # Get the indices of the top_n most similar words (excluding the word itself)\n",
    "    most_similar_indices = word_similarities.argsort()[-top_n-1:-1][::-1]\n",
    "    \n",
    "    # Map indices back to words\n",
    "    similar_words = [ix_to_word[ix] for ix in most_similar_indices]\n",
    "    return similar_words\n",
    "\n",
    "# Assuming you have an inverse dictionary to map indices back to words\n",
    "ix_to_word = {index: word for word, index in word_to_ix.items()}\n",
    "\n",
    "# Choose some words\n",
    "chosen_words = [\"me\", \"white\", \"man\", \"have\", \"be\", \"child\", \"yes\", \"what\"]\n",
    "\n",
    "# Find and print the most similar words for each chosen word\n",
    "for word in chosen_words:\n",
    "    similar_words = find_most_similar_words(word, word_to_ix, ix_to_word, cosine_sim_matrix)\n",
    "    print(f\"Words similar to '{word}': {similar_words}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
